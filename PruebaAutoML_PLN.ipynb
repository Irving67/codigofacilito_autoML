{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Connect to your Worksace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375437422
        }
      },
      "outputs": [],
      "source": [
        "# Librerías para conexión con el Workspace desde cuenta Azure\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    # Se intenta obtener el token por defecto desde el workspace actual\n",
        "    credential = DefaultAzureCredential()\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # En caso de que NO funcionen las credenciales por defecto, se abrirá el \n",
        "    # InteractiveBrowserCredential (para autenticarte desde el navegador)\n",
        "    credential = InteractiveBrowserCredential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375439114
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Cree una instancia cliente para administrar el Worspace\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Preparación de los datos (Transformación)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375442860
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml import Input\n",
        "import mltable\n",
        "\n",
        "# Crea un Dataset basado en los archivos en la carpeta de datos local\n",
        "nombre_tabla = \"Analisis_Sentimientos\"\n",
        "version = \"1\"\n",
        "\n",
        "# Cargamos los datos desde el Asset de datos y los leemos como DataFrame\n",
        "data_asset = ml_client.data.get(nombre_tabla, version=version)\n",
        "tbl = mltable.load(f'azureml:/{data_asset.id}')\n",
        "df = tbl.to_pandas_dataframe()\n",
        "df = df[: 100]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375446910
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Ver la distribución que tenemos de los datos\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Pos = 0\n",
        "Neg = 0\n",
        "\n",
        "for Sentiment in df[\"sentimiento\"]:\n",
        "    if(Sentiment == 'positive'):\n",
        "        Pos += 1\n",
        "    else:\n",
        "        Neg += 1\n",
        "\n",
        "plt.bar(['Positivas', 'Negativas'], [Pos, Neg], color = ['lime', 'red'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375449248
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Preprocesamiento usando nltk\n",
        "import nltk\n",
        "\n",
        "# Descargar stopwords y otros recursos necesarios\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')  # Para tokenizers\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import re  \n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "X = []\n",
        "\n",
        "removedor_tags = re.compile(r'<[^>]+>')\n",
        "\n",
        "sentences = list(df['reseña'])\n",
        "for sen in sentences:\n",
        "\n",
        "    # Filtrado de stopword\n",
        "    for stopword in stop_words:\n",
        "        sentence = sen.replace(\" \" + stopword + \" \", \" \")\n",
        "\n",
        "    # Remover los elementos de HTML (Que aparecen en los comentarios)\n",
        "    sentence = removedor_tags.sub('', sentence)\n",
        "    # Remover espacios múltiples\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    # Convertir todo a minúsculas\n",
        "    sentence = sentence.lower()\n",
        "    # Filtrado de signos de puntuación\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # Tokenización del resultado (Aplicando el rechazo de tokens descrito)\n",
        "    result = tokenizer.tokenize(sentence)\n",
        "    # Agregar al arreglo los textos \"destokenizados\" (Como texto nuevamente)\n",
        "    X.append(TreebankWordDetokenizer().detokenize(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375454072
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Filtrado de más StopWords (Definidas por el usuario)\n",
        "New_StopWords = ['a','acá','ahí','al','algo','algún','alguna','alguno','algunas','algunos','allá','allí','ambos','ante',\n",
        "                 'antes','aquel','aquella','aquello','aquellas','aquellos','aquí','arriba','así','atrás','aun','aunque',\n",
        "                 'bien','cada','casi','como','con','cual','cuales','cualquier','cualquiera','cuan','cuando','cuanto','cuanta',\n",
        "                 'cuantos','cuantas','de','del','demás','desde','donde','dos','el','él','ella','ello','ellas','ellos','en',\n",
        "                 'eres','esa','ese','eso','esas','esos','esta','esto','estas','estos','este','etc','ha','hasta','la','lo','las',\n",
        "                 'los','me','mi','mis','mía','mías','mío','míos','mientras','muy','ni','nosotras','nosotros','nuestra',\n",
        "                 'nuestro','nuestras','nuestros','os','otra','otro','otras','otros','para','pero','pues','que','qué','si','sí',\n",
        "                 'siempre','siendo','sin','sino','so','sobre','sr','sra','sres','sta','su','sus','te','tu','tus','un','una',\n",
        "                 'uno','unas','unos','usted','ustedes','vosotras','vosotros','vuestra','vuestro','vuestras','vuestros','y','ya',\n",
        "                 'yo']\n",
        "\n",
        "for i in range(len(X)):\n",
        "    for element in New_StopWords:\n",
        "        X[i] = X[i].replace(\" \" + str(element) + \" \", \" \")\n",
        "\n",
        "print(\"Textos sin StopWords:\")\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375457420
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creación del vector objetivo (Postivo / Negativo)\n",
        "Sent = df['sentimiento']\n",
        "\n",
        "y = []\n",
        "for Sentimiento in Sent:\n",
        "    if Sentimiento == \"positive\":\n",
        "        y.append(int(1))\n",
        "    else:\n",
        "        y.append(int(0))\n",
        "print(\"Vector de objetivos:\")\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375460037
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Preparamos la capa de embeddingsn(Predefinimos una cantidad de\n",
        "# 5000 palabras consideradas como tokens\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# Transforma cada texto en una secuencia de valores enteros\n",
        "X_train = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Acceder al vocabulario\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Imprimir el vocabulario\n",
        "print(\"Vocabulario:\", word_index)\n",
        "\n",
        "# Si solo quieres las palabras, puedes obtener las claves del diccionario\n",
        "words = list(word_index.keys())\n",
        "print(\"Palabras en el vocabulario:\", words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715376880344
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Guardamos el tokenizador\n",
        "import pickle\n",
        "\n",
        "# Guardar el tokenizador a un archivo\n",
        "with open('./data/sentiments.csv', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375463459
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375465578
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Crear una matriz para el Bag of Words (inicialmente todo a cero)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "bag_of_words = np.zeros((len(X_train), len(word_index)))\n",
        "\n",
        "# Rellenar la matriz con las frecuencias\n",
        "for i, doc in enumerate(X_train):\n",
        "    for word in doc:\n",
        "        index = word - 1  # ajustando el índice porque el índice en Python es base cero\n",
        "        if index < len(word_index):  # solo consideramos las palabras dentro del límite del vocabulario\n",
        "            bag_of_words[i, index] += 1\n",
        "\n",
        "# Crear el DataFrame\n",
        "df = pd.DataFrame(bag_of_words, columns=list(word_index.keys()))\n",
        "\n",
        "# Añadir el vector y como una columna adicional en el DataFrame\n",
        "df['sentiment'] = y\n",
        "\n",
        "# Imprimir el DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715375468847
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Guardar el df en una dirección local\n",
        "local_path = \"\"\n",
        "df.to_csv(local_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": false,
        "gather": {
          "logged": 1715375989432
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": true
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "\n",
        "data = Data(\n",
        "    path=\"\",  # Asegúrate de que el camino sea accesible por Azure\n",
        "    type=\"mltable\",\n",
        "    description=\"Descripción de los datos para analisis de sentimientos\"\n",
        ")\n",
        "\n",
        "registered_data = ml_client.data.create_or_update(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715376186942
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml import Input\n",
        "import mltable\n",
        "\n",
        "# creates a dataset based on the files in the local data folder\n",
        "training_data_input = Input(type=AssetTypes.MLTABLE, path=\"azureml:sentiments:1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715376190182
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import automl\n",
        "\n",
        "# configure the classification job\n",
        "classification_job = automl.classification(\n",
        "    compute=\"InstanciaAutoMLTest\",                 # Nombre del cluster de cómputo\n",
        "    experiment_name=\"AnalisisSentimientosExperimento\", # Nombre que le quieres dar al experimento\n",
        "    training_data=training_data_input,             # Data / Dataset de input\n",
        "    target_column_name=\"sentiment\",                 # Columna objetivo de la clasificación\n",
        "    primary_metric=\"accuracy\",                     # Metrica de evaluación\n",
        "    n_cross_validations=5,                         # Número de validaciones cruzadas\n",
        "    enable_model_explainability=True               # Permite las explicaciones de las predicciones\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715376191971
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# set the limits (optional)\n",
        "classification_job.set_limits(\n",
        "    timeout_minutes=60, \n",
        "    trial_timeout_minutes=20, \n",
        "    max_trials=5,\n",
        "    enable_early_termination=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715376195599
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# set the training properties (optional)\n",
        "classification_job.set_training(\n",
        "    blocked_training_algorithms=[\"LogisticRegression\"], \n",
        "    enable_onnx_compatible_models=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715376198385
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    classification_job\n",
        ")  \n",
        "\n",
        "# submit the job to the backend\n",
        "aml_url = returned_job.studio_url\n",
        "print(\"Monitor your job at\", aml_url)"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
